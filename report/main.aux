\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{10.5555/2621980}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Supervised Learning and Kernel Methods}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Statistical Guarantees vs Computational Bottlenecks}{1}{subsection.2.2}\protected@file@percent }
\citation{rudi2016morenystromcomputationalregularization}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Kernel Ridge Regression applied to $f(x)=\qopname  \relax o{sin}(x)$ (left) and to a localized nonlinear target $f(x)=e^{-(x-2)^2}-0.8\,e^{-(x+2+\qopname  \relax o{sin}(2x))^2}$ (right) using a Gaussian Kernel\relax }}{2}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:KRR_example}{{1}{2}{Kernel Ridge Regression applied to $f(x)=\sin (x)$ (left) and to a localized nonlinear target $f(x)=e^{-(x-2)^2}-0.8\,e^{-(x+2+\sin (2x))^2}$ (right) using a Gaussian Kernel\relax }{figure.caption.3}{}}
\newlabel{eq:LR}{{1}{2}{Statistical Guarantees vs Computational Bottlenecks}{equation.2.1}{}}
\newlabel{Vanilla KRR}{{2}{2}{Statistical Guarantees vs Computational Bottlenecks}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Vanilla KRR Scaling Law on $sin$ function\relax }}{2}{figure.caption.4}\protected@file@percent }
\newlabel{fig:scalling Vanilla KRR}{{2}{2}{Vanilla KRR Scaling Law on $sin$ function\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Nyström Approximation and Reduced Kernel Solvers}{2}{subsection.2.3}\protected@file@percent }
\newlabel{seq:Nystrom}{{2.3}{2}{Nyström Approximation and Reduced Kernel Solvers}{subsection.2.3}{}}
\newlabel{eq:nys}{{3}{2}{Nyström Approximation and Reduced Kernel Solvers}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces KRR with Nyström Approximation using 50 points out of 1000 total\relax }}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:Nystrom toy}{{3}{3}{KRR with Nyström Approximation using 50 points out of 1000 total\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Iterative Solvers and conditioning}{3}{subsection.2.4}\protected@file@percent }
\newlabel{eq:system}{{4}{3}{Iterative Solvers and conditioning}{equation.2.4}{}}
\newlabel{eq:conditioned system}{{5}{3}{Iterative Solvers and conditioning}{equation.2.5}{}}
\newlabel{eq: P definition}{{6}{3}{Iterative Solvers and conditioning}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evolution of condition number of $K_{nm}^\top K_{nm} + \lambda n K_{mm}$\relax }}{3}{figure.caption.6}\protected@file@percent }
\newlabel{fig:conditioning}{{4}{3}{Evolution of condition number of $K_{nm}^\top K_{nm} + \lambda n K_{mm}$\relax }{figure.caption.6}{}}
\newlabel{eq:approximate conditioned system}{{7}{3}{Iterative Solvers and conditioning}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}The Falkon Algorithm}{3}{subsection.2.5}\protected@file@percent }
\newlabel{seq:Falkon}{{2.5}{3}{The Falkon Algorithm}{subsection.2.5}{}}
\newlabel{eq:final sol}{{8}{3}{The Falkon Algorithm}{equation.2.8}{}}
\citation{falkonlibrary2020}
\newlabel{eq:P}{{2.5}{4}{The Falkon Algorithm}{equation.2.8}{}}
\newlabel{eq: conjugate gradient}{{10}{4}{The Falkon Algorithm}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Reproduction of Theoretical Results and Algorithms}{4}{subsection.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Runtime scaling of vanilla KRR, Nyström and FALKON solvers on CPU (log--log scale). Slopes are fitted for $n \ge 10^3$: \relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:runtime-scaling}{{5}{5}{Runtime scaling of vanilla KRR, Nyström and FALKON solvers on CPU (log--log scale). Slopes are fitted for $n \ge 10^3$: \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}RAM memory bottleneck}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of the memory and time consumption for chunking vs non chunking the product $K_{nm}v$ with $v$ a random vector\relax }}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig:memory_chunking}{{6}{6}{Comparison of the memory and time consumption for chunking vs non chunking the product $K_{nm}v$ with $v$ a random vector\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}GPU memory and Multi-GPU support}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Computationnal time and memory cost on a T4 GPU, according to different range of $q$ and $r$ - parameters set to $n = 50000$, $m = 5000$ and $s = d = 20$\relax }}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig:heatmap}{{7}{7}{Computationnal time and memory cost on a T4 GPU, according to different range of $q$ and $r$ - parameters set to $n = 50000$, $m = 5000$ and $s = d = 20$\relax }{figure.caption.9}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Block Cholesky Decomposition (In-Place)\relax }}{7}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimization of data transfers}{7}{subsection.3.3}\protected@file@percent }
\citation{year_prediction_msd_203}
\citation{higgs_280}
\citation{gardner2018gpytorch}
\citation{falkonlibrary2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Other improvements : numerical precision, dealing with thin submatrices and sparse datasets}{8}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Toy experiment : Number of negative values of $||x-y||^2$ with 200 uniform draw $x$ of $\mathcal  {N}(0,1)$ of size $d$ for each point, and $y=x+\epsilon $ with epsilon drawn from $10^{-4}\mathcal  {N}(0,1)$\relax }}{8}{figure.caption.11}\protected@file@percent }
\newlabel{fig:floating_precision}{{8}{8}{Toy experiment : Number of negative values of $||x-y||^2$ with 200 uniform draw $x$ of $\mathcal {N}(0,1)$ of size $d$ for each point, and $y=x+\epsilon $ with epsilon drawn from $10^{-4}\mathcal {N}(0,1)$\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimentation}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Practical details}{8}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Datasets}{8}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Models}{8}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compute}{8}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results}{8}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sanity Check}{8}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling Laws.}{8}{section*.17}\protected@file@percent }
\citation{falkonlibrary2020}
\bibstyle{ACM-Reference-Format}
\bibdata{acmart}
\bibcite{year_prediction_msd_203}{{1}{2011}{{Bertin-Mahieux}}{{}}}
\bibcite{gardner2018gpytorch}{{2}{2018}{{Gardner et~al\mbox  {.}}}{{}}}
\bibcite{falkonlibrary2020}{{3}{2020}{{Meanti et~al\mbox  {.}}}{{}}}
\bibcite{rudi2016morenystromcomputationalregularization}{{4}{2016}{{Rudi et~al\mbox  {.}}}{{}}}
\bibcite{10.5555/2621980}{{5}{2014}{{Shalev-Shwartz and Ben-David}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{0pt}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sanity-check comparison on the \texttt  {mini-HIGGS} dataset. $1-$AUC and total runtime are reported for each method.\relax }}{9}{table.caption.16}\protected@file@percent }
\newlabel{tab:sanity_mini_higgs}{{1}{9}{Sanity-check comparison on the \texttt {mini-HIGGS} dataset. $1-$AUC and total runtime are reported for each method.\relax }{table.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Scaling laws for \textsc  {Falkon}\xspace  on the HIGGS and MSD datasets\relax }}{9}{figure.caption.18}\protected@file@percent }
\newlabel{fig:scaling_laws_falkon}{{9}{9}{Scaling laws for \Falkon on the HIGGS and MSD datasets\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison between \textsc  {Falkon}\xspace  and GPyTorch on the Mini-HIGGS dataset\relax }}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig:scaling_laws_comparison}{{10}{9}{Comparison between \Falkon and GPyTorch on the Mini-HIGGS dataset\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Overall Performance.}{9}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance and runtime comparison on large-scale datasets. Results report test error (relative error for \textsc  {MDS}, \(1-\)AUC for \textsc  {HIGGS}) and total training time. ``\textsc  {Falkon}\xspace  (paper)'' corresponds to the reference values reported in the original publication.\relax }}{9}{table.caption.21}\protected@file@percent }
\newlabel{tab:overall_results}{{2}{9}{Performance and runtime comparison on large-scale datasets. Results report test error (relative error for \textsc {MDS}, \(1-\)AUC for \textsc {HIGGS}) and total training time. ``\Falkon (paper)'' corresponds to the reference values reported in the original publication.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{9}{section*.23}\protected@file@percent }
\newlabel{TotPages}{{9}{9}{}{page.9}{}}
\gdef \@abspage@last{9}
